{"name":"Spark standalone cluster tutorial","tagline":"Spark from the ground up","body":"Getting started with Spark\r\n-----\r\n\r\nThis tutorial was written in _October 2013._  \r\nAt the time, the current development version of _Spark_ was **0.9.0.**  \r\n\r\n_The tutorial covers Spark setup on Ubuntu 12.04:_\r\n- installation of all Spark prerequisites\r\n- Spark build and installation\r\n- basic Spark configuration\r\n- standalone cluster setup (one master and 4 slaves on a single machine)\r\n- running the `math.PI` approximation job on a standalone cluster\r\n\r\n### My setup\r\n_Before installing Spark:_\r\n- Ubuntu 12.04 LTS 32-bit\r\n- OpenJDK 1.6.0_27\r\n- Scala 2.9.3\r\n- Maven 3.0.4\r\n- Python 2.7.3 (you already have this)\r\n- Git 1.7.9.5  (and this, I presume)\r\n  \r\n  \r\nThe official one-liner describes Spark as \"a general purpose cluster computing platform\".  \r\n\r\nSpark was conceived and developed at Berkeley labs. It is currently incubated at Apache and improved and maintained by a rapidly growing community of users, thus it's expected to graduate to top-level project very soon.  \r\nIt's written mainly in Scala, and provides Scala, Java and Python APIs.  \r\nScala API provides a way to write concise, higher level routines that effectively manipulate distributed data.  \r\n\r\nHere's a quick example of how straightforward it is to distribute some arbitrary data with Scala API:\r\n\r\n```scala\r\n// parallelized collection example\r\n// example used Scala interpreter (also called 'repl') as an interface to Spark\r\n\r\n// declare scala array:\r\nscala> val data = Array(1, 2, 3, 4, 5)\r\n\r\n// Scala interpreter responds with:\r\ndata: Array[Int] = Array(1, 2, 3, 4, 5)\r\n\r\n// distribute the array in a cluster:\r\nscala> val distData = sc.parallelize(data)\r\n\r\n// sc is an instance of SparkCluster that's initialized for you by Scala repl\r\n\r\n// returns Resilient Distributed Dataset:\r\ndistData: spark.RDD[Int] = spark.ParallelCollection@88eebe8e  // repl output\r\n```\r\n\r\n### What the hell is RDD?\r\n**Resilient Distributed Dataset** is a collection that has been distributed _all over_ the Spark cluster. RDDs' main purpose is to support higher-level, parallel operations on data in a straightforward manner.  \r\nThere are currently two types of RDDs: **parallelized collections**, which take an existing Scala collection and run operations on it in parallel, and **Hadoop** datasets, which run functions on each record of a file in _HDFS_ (or any other storage system supported by _Hadoop_).\r\n\r\n## Prereqs\r\nFirst, let's get the requirements out of the way.  \r\n\r\n### Install Oracle's JDK6\r\nSince Oracle, after they acquired Java from Sun, changed license agreement (not in a good way), Canonical no longer provides packages for Oracle's Java. Since we need oracle's version of java, we'll need to install it and set it to be the default JVM on our system.  \r\n\r\n> We probably don't need Oracle's Java, but I had some weird problems while building Spark with OpenJDK - presumably due to the fact that open-jdk places jars in `/usr/share/java`, I'm not really sure, but installation of oracle-java effectively solved all those problems, so I haven't investigated any further what exactly happened there.\r\n\r\nHere's how:  \r\n\r\n```sh\r\n# you may or may not want to remove open-jdk (not necessary):\r\nsudo apt-get purge openjdk*\r\n\r\n# to add PPA source repository to apt:\r\nsudo add-apt-repository ppa:webupd8team/java\r\n\r\n# to refresh the sources list:\r\nsudo apt-get update\r\n\r\n# to install JDK 6:\r\nsudo apt-get install oracle-java6-installer\r\n```\r\n\r\nThat should install java and make it default, all in one go.  \r\nIf you check with:  \r\n\r\n```sh\r\ncd /etc/alternatives\r\nls -lat\r\n```\r\n\r\n... you should see that `java` symlink points to JRE, that's inside our newly installed JDK:\r\n\r\n![java default](https://raw.github.com/mbonaci/mbo-spark/master/resources/java-default.PNG)\r\n\r\nIf you see some other java version as default, you can fix that by executing either of these:\r\n\r\n```sh\r\n# to make oracle's jdk6 the default:\r\nsudo apt-get install oracle-java6-set-default\r\n\r\n# the other way of setting default java is:\r\nsudo update-alternatives --config java\r\n\r\n# are we good?\r\nfile `which java`\r\n# should return:\r\n# /usr/bin/java: symbolic link to `/etc/alternatives/java'\r\n\r\nfile /etc/alternatives/java\r\n# should return:\r\n# /etc/alternatives/java: symbolic link to `/usr/lib/jvm/java-6-oracle/jre/bin/java'\r\n```\r\n\r\n**Check/set `JAVA_HOME`**  \r\n\r\nFor a good measure, I like to set `JAVA_HOME` explicitly (_Spark_ checks for its existence).\r\n\r\n```sh\r\n# check where it currently points\r\necho $JAVA_HOME\r\n# if you're doing this on a fresh machine\r\n# and you just installed Java for the first time\r\n# JAVA_HOME should not be set (you get an empty line when you echo it)\r\n\r\n################## either set it system wide ####################\r\nsudo echo \"JAVA_HOME=/usr/lib/jvm/java-6-oracle\" >> /etc/environment\r\n\r\n# to reload\r\nsource /etc/environment\r\n\r\n################## or only for the current user #################\r\necho \"JAVA_HOME=/usr/lib/jvm/java-6-oracle/\" >> ~/.pam_environment\r\n\r\n# to reload\r\nsource ~/.pam_environment\r\n\r\n#################################################################\r\n\r\n# now try it out\r\necho $JAVA_HOME\r\n```\r\n\r\n> `.pam_environment` is the new `.bashrc`. [What?](https://help.ubuntu.com/community/EnvironmentVariables#Session-wide_environment_variables)\r\n  \r\n> Having problems with Java setup? Check the [latest Ubuntu Java documentation](https://help.ubuntu.com/community/Java).\r\n\r\n### Install Scala\r\n- download 2.9.3 binaries for your OS from [here](http://www.scala-lang.org/download/2.9.3.html) (don't click the download link on top, scroll down to find `deb` download link, or simply use this [direct download link](http://www.scala-lang.org/files/archive/scala-2.9.3.deb))\r\n- the easiest way to install Scala from `deb` package is to simply double click on it, and let the _Ubuntu Software Center_ take care of you\r\n\r\n```sh\r\n# to check whether the installation went well:\r\nscala -version\r\n\r\n# should spit out:\r\nScala code runner version 2.9.3 -- Copyright 2002-2011, LAMP/EPFL\r\n```\r\n\r\n**Check/set `SCALA_HOME`**  \r\n\r\n```sh\r\n# I'll set SCALA_HOME only for my user sessions\r\n# who knows, maybe my wife will like to use another version :)\r\necho \"SCALA_HOME=/usr/share/java\" >> ~/.pam_environment\r\n\r\n# again, same as with java, to reload:\r\nsource ~/.pam_environment\r\n```\r\n\r\n### Install Maven\r\n_Skip if you'll choose to install Spark from binaries (see the next section for more info)_\r\nThis one is dead simple: \r\n\r\n```sh\r\nsudo apt-get install maven\r\n```\r\n\r\nBe warned, a large download will take place.  \r\nIt is flat out awful that a dependency & build management tool may become so bloated that it **weighs 146MB**, but that's a whole [different story](https://github.com/mbonaci/mbo-storm/wiki/Storm-setup-in-Eclipse-with-Maven,-Git-and-GitHub#a-note-about-maven)...\r\n\r\n## Install Spark\r\nWe'll try, like a couple of hoodlums, to build the cutting edge, development version of Spark ourselves. Screw binaries, right :)  \r\n  \r\nIf you were to say that this punk move will just complicate things, you wouldn't be far from truth. So, if you'd like to simplify things a bit, and avoid possible alpha-version bugs down the road, go ahead and download (and install) Spark binaries from [here](http://spark.incubator.apache.org/downloads.html).  \r\nIf, on the other hand, you're not scared, keep following instructions.  \r\nJust kidding.  \r\nNo, honestly, I really suggest that you use binaries (and skip to [OMG! section](#omg-i-have-a-running-spark-in-my-home)).\r\n\r\n> Back to the story, so my grandma was building Spark from source the other day and she noticed a couple of build errors...\r\n\r\n- to get the Spark source code:\r\n\r\n```sh\r\n# clone the development repo:\r\ngit clone git://github.com/apache/incubator-spark.git\r\n\r\n# rename the folder:\r\nmv incubator-spark spark\r\n\r\n# go into it:\r\ncd spark\r\n```\r\n\r\n- we'll use _Maven_ to build _Spark_:\r\n\r\n> But what these params bellow actually mean?  \r\n> Spark will build against Hadoop 1.0.4 by default, so if you want to read from HDFS (optional), use your version of Hadoop. If not, choose any version.  \r\n> For more options and additional details, take a look at the official [ instructions on building Spark with Maven](http://spark.incubator.apache.org/docs/latest/building-with-maven.html).\r\n\r\n```sh\r\n# first, to avoid the notorious 'permgen' error\r\n# increase the amount memory available to the JVM:\r\nexport MAVEN_OPTS=\"-Xmx1300M -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m\"\r\n\r\n# then trigger the build:\r\nmvn -Phadoop2-yarn -Dhadoop.version=2.0.5-alpha -Dyarn.version=2.0.5-alpha -DskipTests clean package\r\n```\r\n\r\n> If you did everything right, the build process should complete without a glitch, in about 15 to 30 minutes (downloads take the majority of that time), depending on your hardware. The only type of notifications should be Scala deprecation and duplicate class warnings, but both can be ignored.\r\n\r\nYou should see something like this by the end of the compilation process:\r\n\r\n![spark-build-success](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-build-success.png)\r\n\r\nThe above is from Ubuntu, with Core-i7-Quad 2.6GHz and 8GB of RAM.  \r\nAnd this is the same thing from Xubuntu, with Core-i5-Duo 3.2GHz and 4GB of RAM:\r\n\r\n![spark-build-success](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-build-success-xubuntu.png)\r\n\r\n> Since I have successfully built Spark with _mvn_ I have never used _sbt (Scala Build Tool)_ to build it, but that option is also available to you.\r\n\r\n## OMG! I have a running Spark in my home\r\n\r\n> ... IN MY HOME, in my bedroom, where my wife sleeps! Where my children come to play with their toys. In my home.\r\n\r\nYes, that was my general feeling when I first connected to a Spark master with Scala repl:\r\n\r\n![spark-repl](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-repl.png)\r\n\r\n### How to get there\r\n\r\nOK, we installed all the prerequisites and successfully built Spark. Now, it's finally time to have some fun with it.\r\n\r\nTo get the ball rolling, start the Spark master:\r\n\r\n```sh\r\n# to start the Spark master on your localhost:\r\n./bin/start-master.sh\r\n\r\n# outputs master's class and log file info:\r\nstarting org.apache.spark.deploy.master.Master, logging to /home/mbo/spark/bin/../\r\nlogs/spark-mbo-org.apache.spark.deploy.master.Master-1-mbo-ubuntu-vbox.out\r\n```\r\n\r\nIf you need more info on how the startup process works, take a look [here](http://spark.incubator.apache.org/docs/latest/spark-standalone.html).  \r\n  \r\nTo check out master's web console, open http://localhost:8080/. Nice, ha?\r\n\r\n![master's web console](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-web-console.png)\r\n\r\n### Starting slave workers\r\n\r\n> Our beloved master is lonely, with no one to boss around. Let's fix that.\r\n\r\nSpark _master_ requires passwordless `ssh` login to its _slaves_, and since we're building a standalone Spark cluster, we'll need to facilitate _localhost to localhost_ passwordless connection.\r\n\r\nIf your private key has a password, you'll need to generate a new key and copy its public part to `~/.ssh/authorized_keys`:\r\n\r\n```sh\r\n# generate new public-private key pair:\r\nssh-keygen\r\n\r\n# just press enter both times, when asked for password\r\n\r\n# to add your key to authorized keys list on your machine:\r\nssh-copy-id mbo@mbo-xubuntu\r\n\r\n# where 'mbo' is your username and 'mbo-xubuntu' is your localhost's name\r\n# confirm by typing 'yes' and then enter your login password, when asked\r\n```\r\n\r\nThat should look similar to this:\r\n\r\n![ssh-keygen](https://raw.github.com/mbonaci/mbo-spark/master/resources/ssh-keygen-xubuntu.PNG)\r\n\r\n> If you get stuck, follow [these instructions](http://help.ubuntu.com/12.04/serverguide/openssh-server.html#openssh-keys), and [these](http://askubuntu.com/a/296574/116447), if needed.\r\n  \r\n> Be careful not to open a door for malicious intrusion attempts. If you're new to `ssh`, [here](https://help.ubuntu.com/community/SSH) is a short and sweet intro to _openssh_.\r\n  \r\nIf you don't have `ssh` server installed, you'll need to get one:\r\n\r\n```sh\r\n# to check whether the openssh server is installed\r\nservice ssh status\r\n\r\n# if that returns \"unrecognized service\", then follow instructions:\r\n\r\n# to install openssh-server (most Ubuntu versions come only with ssh client)\r\nsudo apt-get install openssh-server\r\n\r\n# then check whether the openssh server is up (should be automatically started)\r\nservice ssh status\r\n\r\n# if not\r\nservice ssh start\r\n```\r\n\r\n### Actually starting slave workers\r\n\r\nTo tell Spark to run 4 workers on each slave machine, we'll create a new config file:\r\n\r\n```sh\r\n# create spark-env.sh file using the provided template:\r\ncp ./conf/spark-env.sh.template ./conf/spark-env.sh\r\n\r\n# append a configuration param to the end of the file:\r\necho \"export SPARK_WORKER_INSTANCES=4\" >> ./conf/spark-env.sh\r\n```\r\n\r\n> Now we've hopefully prepared everything to finally launch 4 slave workers, on the same machine where our master is already lurking around (slaves will be assigned random ports, both for _repl_ and _http_ access, thus avoiding port collision).\r\n\r\n```sh\r\n# to start slave workers:\r\n./bin/start-slaves.sh\r\n```\r\n\r\nYou should see something similar to this:\r\n\r\n![4 slaves start](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-4-slaves-started.png)\r\n\r\nIf you now refresh master's web console, you should see 4 slaves listed there:\r\n\r\n![4 slaves in web console](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-master-web-console-4-slaves.png)\r\n\r\nClicking on a slave's link opens its web console:\r\n\r\n![slave web console](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-slaves-web-console.png)\r\n\r\n### Starting and stopping the whole cluster\r\n\r\nSince you're going to start and stop master and slaves multiple times, let's quickly go through the simplified procedure of starting and stopping the whole cluster with one command:\r\n\r\n```sh\r\n# first, let's stop the master and all the slaves:\r\n./bin/stop-all.sh\r\n\r\n# then, to start all of them in one go:\r\n./bin/start-all.sh\r\n```\r\n\r\n## Configuring _log4j_\r\nI regularly configure log4j to write to a log file, instead of a console.\r\nIf you'd like to set up the same thing yourself, you'll need to modify `log4j.properties` located in `spark/conf/` directory. Something like this:\r\n\r\n```sh\r\n# Initialize root logger\r\nlog4j.rootLogger=INFO, FILE\r\n# Set everything to be logged to the console\r\nlog4j.rootCategory=INFO, FILE\r\n\r\n# Ignore messages below warning level from Jetty, because it's a bit verbose\r\nlog4j.logger.org.eclipse.jetty=WARN\r\n\r\n# Set the appender named FILE to be a File appender\r\nlog4j.appender.FILE=org.apache.log4j.FileAppender\r\n\r\n# Change the path to where you want the log file to reside\r\nlog4j.appender.FILE.File=/mbo/spark/logs/SparkOut.log\r\n\r\n# Prettify output a bit\r\nlog4j.appender.FILE.layout=org.apache.log4j.PatternLayout\r\nlog4j.appender.FILE.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\r\n```\r\n\r\nTo assure that Spark will be able to find `log4j.properties`, I suggest you create a new `java-opts` file in `spark/conf/` directory and place this line inside (modify the path, of course):\r\n\r\n```sh\r\n-Dlog4j.configuration=file:///mbo/spark/conf/log4j.properties\r\n```\r\n\r\n> FYI, the same thing can be accomplished using `SPARK_JAVA_OPTS` param in `spark-env.sh`.\r\n\r\n## Playing with Spark\r\n\r\nWe are finally ready for our first interactive session with Spark.  \r\nTo launch Spark Scala repl:\r\n\r\n```sh\r\n# launch scala repl\r\nMASTER=spark://localhost:7077 ./spark-shell\r\n```\r\n\r\n![spark repl](https://raw.github.com/mbonaci/mbo-spark/master/resources/spark-repl.png)\r\n\r\n### Hello Spark\r\n\r\nAs a demonstration, let's use our Spark cluster to approximate _PI_ using [MapReduce algorithm]():  \r\n\r\n```scala\r\n/* throwing darts and examining coordinates */\r\nval NUM_SAMPLES = 100000\r\nval count = spark.parallelize(1 to NUM_SAMPLES).map(i =>\r\n  val x = Math.random\r\n  val y = Math.random\r\n  if (x * x + y * y < 1) 1.0 else 0.0\r\n).reduce(_ + _)\r\n\r\nprintln(\"Pi is roughly \" + 4 * count / NUM_SAMPLES)\r\n```\r\n\r\nIf you're not comfortable with Scala, I recently wrote a [Java developer's Scala cheat sheet](http://mbonaci.github.io/scala/) (based on Programming in Scala SE book, by Martin Odersky, whose first edition is freely available [online](http://www.artima.com/pins1ed/)), which is basically a big reference card, where you can look up almost any Scala topic you come across.\r\n\r\n## Setting up Spark development environment in Eclipse\r\n\r\nMost Spark developers use IntelliJ IDEA, but since I don't have enough will power to switch over, I'm still with Eclipse (when it comes to JVM-related development, that is).  \r\nAll you need to do to prepare Spark for Eclipse is run:\r\n\r\n```sh\r\nsbt/sbt eclipse\r\n```\r\n\r\n![sbt eclipse](https://raw.github.com/mbonaci/mbo-spark/master/resources/sbt-eclipse.PNG)\r\n\r\nWhich should hopefully finish like this:\r\n\r\n![sbt eclipse success](https://raw.github.com/mbonaci/mbo-spark/master/resources/sbt-eclipse-success.PNG)\r\n\r\n## What Spark uses under the hood?\r\n\r\nSince you're still reading this, I'll go ahead and assume that you're like me. You don't want to **just use** something, you like to know what you're working with and how it all works together.  \r\n..\r\nSo let me present you with the list of software (all open source) that was installed automatically, alongside Spark (I wrote a short explanation besides ones that were unknown to me):\r\n\r\n### My setup - _post festum_\r\n- Hadoop 2.0.5-alpha (with Yarn)\r\n- Zookeeper 3.4.5\r\n- Avro 1.7.4\r\n- Akka 2.0.5\r\n- Lift-json 2.5\r\n- Twitter Chill 2.9.3 (Kryo serialization for Scala)\r\n- Jackson 2.1 (REST)\r\n- Google Protobuf 2.4.1\r\n- Scala-arm (Scala automatic resource management)\r\n- Objenesis 1.2 (DI & serialization)\r\n- Jetty 7.6.8\r\n- ASM 4.0 (bytecode manipulation)\r\n- Minlog & ReflectASM (bytecode helpers)\r\n- FindBugs 1.3.9\r\n- Guava (Google's Java general utils)\r\n- ParaNamer 2.3 (Java method parameter names access)\r\n- Netty 4.0\r\n- Ning-compress-LZF 0.8.4 (LZF for Java)\r\n- H2-LZF 1.0 (LinkedIn's LZF compression, used by Voldemort)\r\n- Snappy-for-java 1.0.5\r\n- Ganglia  (cluster monitoring)\r\n- Metrics 3.0 (collector for Ganglia)\r\n- GMetric4j (collector for Ganglia)\r\n- FastUtil 6.4.4 (improved Java collections)\r\n- Jets3t 0.7.1 (Amazon S3 Java tools)\r\n- RemoteTea 1.0.7 (ONC/RPC over TCP and UDP for Java)\r\n- Xmlenc 0.52 (streaming XML in Java)\r\n- Typesafe config 0.3.1 (JVM config files tools)\r\n- Dough Lea's concurrent (advanced util.concurrent)\r\n- Colt 1.2.0 (math/stat/analysis lib used at Cern)\r\n- Apache Commons, Velocity, Log4j, Slf4j\r\n  \r\n  \r\n_Interesting topics to cover:_  \r\n\r\n**Spark family:**  \r\n - [Mesos](http://mesos.apache.org/)\r\n - Bagel (graph computations)\r\n - MLlib (machine learning)\r\n - Streaming\r\n\r\n**Spark integration with:**  \r\n - [Hadoop](http://hadoop.apache.org/)\r\n - [Kafka](http://kafka.apache.org/)\r\n - [Kiji](http://www.kiji.org/)\r\n - [Storm](http://storm-project.org)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}